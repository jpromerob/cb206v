{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "from tools import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining MLP Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, num_classes, hidden_layers):\n",
    "        super(MLP, self).__init__()\n",
    "        \n",
    "        # Create a list to hold the layers\n",
    "        layers = []\n",
    "        \n",
    "        # Input layer to first hidden layer\n",
    "        layers.append(nn.Linear(input_size, hidden_layers[0]))\n",
    "        layers.append(nn.ReLU())  # Add activation after first hidden layer\n",
    "        \n",
    "        # Add hidden layers\n",
    "        for i in range(1, len(hidden_layers)):\n",
    "            layers.append(nn.Linear(hidden_layers[i-1], hidden_layers[i]))\n",
    "            layers.append(nn.ReLU())  # Add activation after hidden layers\n",
    "        \n",
    "        # Add the output layer\n",
    "        layers.append(nn.Linear(hidden_layers[-1], num_classes))\n",
    "        \n",
    "        # Use nn.Sequential to combine layers\n",
    "        self.network = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load your data\n",
    "merged_dataset = pd.read_csv('datasets/merged_dataset.csv', low_memory=False)  # Assuming your data is in a CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " peakStart peakEnd peakName peakScore FGRstart FGRend FGRstrand region\n"
     ]
    }
   ],
   "source": [
    "nb_ATAC_seq_rows = 8\n",
    "ATAC_seq_names = merged_dataset.columns[:nb_ATAC_seq_rows]\n",
    "all_names = \"\"\n",
    "for name in ATAC_seq_names:\n",
    "    all_names += f' {name}'\n",
    "print(all_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10812\n",
      "9896\n",
      "47942\n"
     ]
    }
   ],
   "source": [
    "# Regions of interest\n",
    "\n",
    "reg_interest = ['promoter', 'enhancer', 'geneBody']\n",
    "reg_ix_interest = []\n",
    "min_common_occ_nb = 1000000\n",
    "for region in reg_interest:\n",
    "    occ_reg = (merged_dataset['region']==region_dict[region]).sum()\n",
    "    if occ_reg < min_common_occ_nb:\n",
    "        min_common_occ_nb = occ_reg\n",
    "    print((merged_dataset['region']==region_dict[region]).sum())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_MLP(mlp_dataset, min_common_occ_nb, roi_list,  nb_runs, list_l_rate, nb_hidden_layers, list_nb_hn_1, list_nb_hn_2, summary):\n",
    "    \n",
    "    compendium_rows = nb_runs*len(list_l_rate)*len(list_nb_hn_1)*len(list_nb_hn_2)\n",
    "    compendium = array_zeros = np.zeros((compendium_rows, 2+nb_hidden_layers+1)) # 3 as in: run, l_rate | 1 as in: performance\n",
    "\n",
    "    compendium_idx = 0\n",
    "    best_mlp_score = 0\n",
    "    best_loss = 10\n",
    "\n",
    "\n",
    "    # Define the number of samples for each region\n",
    "    N = int(min_common_occ_nb*0.8)  # Adjust this value as needed\n",
    "\n",
    "    # Select N random rows for each region\n",
    "    sample_list = []\n",
    "    for roi in roi_list:\n",
    "        sample_list.append(mlp_dataset[mlp_dataset['region'] == region_dict[roi]].sample(n=N, random_state=42))\n",
    "\n",
    "    # Concatenate the sampled rows to create the final dataset\n",
    "    balanced_dataset = pd.concat(sample_list, ignore_index=True)\n",
    "\n",
    "    X = balanced_dataset.drop(['peakStart', 'peakEnd', 'peakName', 'peakScore', 'FGRstart', 'FGRend', 'FGRstrand', 'region'], axis=1).values  # Features\n",
    "    y = balanced_dataset['region'].values  # Target variable\n",
    "\n",
    "\n",
    "    # Split the dataset\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "    # Standardize the features\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "    # Convert data to PyTorch tensors\n",
    "    X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "    y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "    X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "    y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "    input_size = X_train.shape[1]\n",
    "\n",
    "    for run_idx in range(nb_runs):\n",
    "        for l_rate in list_l_rate:\n",
    "            for nb_hn_1 in list_nb_hn_1:\n",
    "                for nb_hn_2 in list_nb_hn_2:\n",
    "                    list_nb_hn = [nb_hn_1, nb_hn_2]\n",
    "\n",
    "\n",
    "                    # Instantiate the model\n",
    "                    model = MLP(input_size, len(roi_list), list_nb_hn)  # Use the dynamic MLP model\n",
    "\n",
    "                    # Define the loss function and optimizer\n",
    "                    criterion = nn.CrossEntropyLoss()  # Suitable for multi-class classification\n",
    "                    optimizer = optim.Adam(model.parameters(), lr=l_rate*0.001)  # Adam optimizer\n",
    "\n",
    "                    # Train the model\n",
    "                    num_epochs = 500\n",
    "                    \n",
    "                    loss_vs_epoch = []\n",
    "                    for epoch in range(num_epochs):\n",
    "                        model.train()\n",
    "                        optimizer.zero_grad()  # Clear the gradients\n",
    "                        outputs = model(X_train_tensor)  # Forward pass\n",
    "                        loss = criterion(outputs, y_train_tensor)  # Compute loss\n",
    "                        loss.backward()  # Backward pass\n",
    "                        optimizer.step()  # Update weights\n",
    "\n",
    "                        if (epoch + 1) % 500 == 0:  # Print loss every 10 epochs\n",
    "                            print(f'Run {compendium_idx+1}/{compendium.shape[0]} : Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "                        loss_vs_epoch.append(loss.item())\n",
    "\n",
    "\n",
    "                    # Evaluate the model\n",
    "                    model.eval()\n",
    "                    with torch.no_grad():\n",
    "                        y_pred = model(X_test_tensor)  # Forward pass\n",
    "                        _, predicted_classes = torch.max(y_pred, 1)  # Get predicted class labels\n",
    "\n",
    "                    # Print evaluation metrics\n",
    "                    report_dict = classification_report(y_test, predicted_classes.numpy(), target_names=roi_list, output_dict=True, zero_division=0)\n",
    "\n",
    "                    # Store the precision values in a variable\n",
    "                    precisions = {key: value['precision'] for key, value in report_dict.items() if key != 'accuracy'}\n",
    "                    recalls = {key: value['recall'] for key, value in report_dict.items() if key != 'accuracy'}\n",
    "                    f1_scores = {key: value['f1-score'] for key, value in report_dict.items() if key != 'accuracy'}\n",
    "                    supports = {key: value['support'] for key, value in report_dict.items() if key != 'accuracy'}\n",
    "\n",
    "                    mlp_score = 0\n",
    "                    for roi in roi_list:\n",
    "                        mlp_score += f1_scores[roi]\n",
    "                    mlp_score = mlp_score/len(roi_list)\n",
    "\n",
    "\n",
    "                    if mlp_score > best_mlp_score:\n",
    "                    # if loss.item() < best_loss:\n",
    "                        best_y_test = y_test\n",
    "                        best_predicted_classes = predicted_classes\n",
    "                        best_loss_vs_epoch = loss_vs_epoch\n",
    "                        best_mlp_score = mlp_score\n",
    "                        best_loss = loss.item()\n",
    "\n",
    "\n",
    "                    compendium[compendium_idx] = [run_idx, l_rate, nb_hn_1, nb_hn_2, mlp_score] \n",
    "                    compendium_idx += 1\n",
    "        \n",
    "    if summary:\n",
    "        print(classification_report(y_test, predicted_classes.numpy(), target_names=reg_interest, zero_division=0))\n",
    "        plot_results(nb_hidden_layers, reg_interest, best_y_test, best_predicted_classes, best_loss_vs_epoch, compendium, l_rate)\n",
    "        \n",
    "    return compendium"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid Search for 2 Hidden Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 1/4 : Epoch [500/500], Loss: 0.7764\n",
      "Run 2/4 : Epoch [500/500], Loss: 0.7701\n",
      "Run 3/4 : Epoch [500/500], Loss: 0.7855\n",
      "Run 4/4 : Epoch [500/500], Loss: 0.7752\n"
     ]
    }
   ],
   "source": [
    "\n",
    "nb_runs = 4\n",
    "nb_hidden_layers = 2\n",
    "list_l_rate  = [10] #[10,20]\n",
    "list_nb_hn_1 = [4] #[4,8,12,16]\n",
    "list_nb_hn_2 = [4] #[4,8,12,16]\n",
    "summary = False\n",
    "\n",
    "compendium = train_MLP(merged_dataset, min_common_occ_nb, reg_interest,  nb_runs, list_l_rate, nb_hidden_layers, list_nb_hn_1, list_nb_hn_2, summary)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best combination:\n",
      "l_rate            10.000000\n",
      "nb_hn_1            4.000000\n",
      "nb_hn_2            4.000000\n",
      "mean_mlp_score     0.642844\n",
      "std_mlp_score      0.009130\n",
      "Name: 0, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(compendium, columns=['run_idx', 'l_rate', 'nb_hn_1', 'nb_hn_2', 'mlp_score'])\n",
    "\n",
    "# Group by the unique combinations of 'l_rate', 'nb_hn_1', and 'nb_hn_2'\n",
    "grouped_df = df.groupby(['l_rate', 'nb_hn_1', 'nb_hn_2']).agg(\n",
    "    mean_mlp_score=('mlp_score', 'mean'),\n",
    "    std_mlp_score=('mlp_score', 'std')\n",
    ").reset_index()\n",
    "\n",
    "# Find the combination with the highest mean score\n",
    "best_combination = grouped_df.loc[grouped_df['mean_mlp_score'].idxmax()]\n",
    "\n",
    "print(\"Best combination:\")\n",
    "print(best_combination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_247866/3797287149.py:1: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  best_combination[0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10.0"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_combination[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_247866/446483933.py:3: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  list_l_rate  = [best_combination[0]]\n",
      "/tmp/ipykernel_247866/446483933.py:4: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  list_nb_hn_1 = [best_combination[1]]\n",
      "/tmp/ipykernel_247866/446483933.py:5: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  list_nb_hn_2 = [best_combination[2]]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "empty() received an invalid combination of arguments - got (tuple, dtype=NoneType, device=NoneType), but expected one of:\n * (tuple of ints size, *, tuple of names names, torch.memory_format memory_format = None, torch.dtype dtype = None, torch.layout layout = None, torch.device device = None, bool pin_memory = False, bool requires_grad = False)\n * (tuple of ints size, *, torch.memory_format memory_format = None, Tensor out = None, torch.dtype dtype = None, torch.layout layout = None, torch.device device = None, bool pin_memory = False, bool requires_grad = False)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[63], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m list_nb_hn_2 \u001b[38;5;241m=\u001b[39m [best_combination[\u001b[38;5;241m2\u001b[39m]] \n\u001b[1;32m      6\u001b[0m summary \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m \u001b[43mtrain_MLP\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmerged_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_common_occ_nb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreg_interest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[43mnb_runs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlist_l_rate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnb_hidden_layers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlist_nb_hn_1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlist_nb_hn_2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msummary\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[59], line 49\u001b[0m, in \u001b[0;36mtrain_MLP\u001b[0;34m(mlp_dataset, min_common_occ_nb, roi_list, nb_runs, list_l_rate, nb_hidden_layers, list_nb_hn_1, list_nb_hn_2, summary)\u001b[0m\n\u001b[1;32m     45\u001b[0m list_nb_hn \u001b[38;5;241m=\u001b[39m [nb_hn_1, nb_hn_2]\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# Instantiate the model\u001b[39;00m\n\u001b[0;32m---> 49\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mMLP\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mroi_list\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlist_nb_hn\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Use the dynamic MLP model\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m# Define the loss function and optimizer\u001b[39;00m\n\u001b[1;32m     52\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()  \u001b[38;5;66;03m# Suitable for multi-class classification\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[55], line 9\u001b[0m, in \u001b[0;36mMLP.__init__\u001b[0;34m(self, input_size, num_classes, hidden_layers)\u001b[0m\n\u001b[1;32m      6\u001b[0m layers \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Input layer to first hidden layer\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m layers\u001b[38;5;241m.\u001b[39mappend(\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLinear\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_layers\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     10\u001b[0m layers\u001b[38;5;241m.\u001b[39mappend(nn\u001b[38;5;241m.\u001b[39mReLU())  \u001b[38;5;66;03m# Add activation after first hidden layer\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Add hidden layers\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/linear.py:99\u001b[0m, in \u001b[0;36mLinear.__init__\u001b[0;34m(self, in_features, out_features, bias, device, dtype)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_features \u001b[38;5;241m=\u001b[39m in_features\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_features \u001b[38;5;241m=\u001b[39m out_features\n\u001b[0;32m---> 99\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight \u001b[38;5;241m=\u001b[39m Parameter(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mempty\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_features\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfactory_kwargs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m bias:\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias \u001b[38;5;241m=\u001b[39m Parameter(torch\u001b[38;5;241m.\u001b[39mempty(out_features, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfactory_kwargs))\n",
      "\u001b[0;31mTypeError\u001b[0m: empty() received an invalid combination of arguments - got (tuple, dtype=NoneType, device=NoneType), but expected one of:\n * (tuple of ints size, *, tuple of names names, torch.memory_format memory_format = None, torch.dtype dtype = None, torch.layout layout = None, torch.device device = None, bool pin_memory = False, bool requires_grad = False)\n * (tuple of ints size, *, torch.memory_format memory_format = None, Tensor out = None, torch.dtype dtype = None, torch.layout layout = None, torch.device device = None, bool pin_memory = False, bool requires_grad = False)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "nb_runs = 4\n",
    "nb_hidden_layers = 2\n",
    "list_l_rate  = [int(best_combination[0])] \n",
    "list_nb_hn_1 = [int(best_combination[1])] \n",
    "list_nb_hn_2 = [int(best_combination[2])] \n",
    "summary = True\n",
    "train_MLP(merged_dataset, min_common_occ_nb, reg_interest,  nb_runs, list_l_rate, nb_hidden_layers, list_nb_hn_1, list_nb_hn_2, summary)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'{int(0.34*100*y.shape[0]/merged_dataset.shape[0])}')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
